tag: ows-pl2

actor:
  team: OpenWebSearch

research goal:
  description: |
    A PyTerrier baseline that uses the ir-datasets integration for LongEval.
    
    We use PL2 and leave everything to the defaults of PyTerrier.
    I.e., no training data is used at all.

platform:
  software:
    # Which software and tools did you use for training, tunning and running your system?
    # You can maintain the software that you used manually.
    # Alternatively, you can use repro_eval or the tirex_tracker to track this.
    libraries:
      - PyTerrier 0.10.0

implementation:
  source:
    # Please provide a reference to your code if possible.
    # If you can not share your code, you can delete the implementation section.
    # The repro_eval and/or tirex_tracker tools can track this automatically, including commits, branches, etc.
    repository: https://github.com/OpenWebSearch/LONGEVAL-25/

data:
  # Please describe which training data your system used, e.g., longeval-sci, longeval-web, MS MARCO, etc.
  training data:
    - name: Nothing

method:
  # Boolean value indicating if it is a automatic (true) or manual (false) run
  automatic: true

  indexing:
    tokenizer: Default from PyTerrier
    stemmer: Default from PyTerrier
    stopwords: Default from PyTerrier

  retrieval:
      name: PL2

      ##################################################
      # Yes/No Questions
      ##################################################

      # Did you use any statistical ranking model? (yes/no)
      lexical: yes

      # Did you use any deep neural network model? (yes/no)
      deep_neural_model: no

      # Did you use a sparse neural model? (yes/no):
      sparse_neural_model: no

      # Did you use a dense neural model? (yes/no):
      dense_neural_model: no

      # Did you use more than a single retrieval model? (yes/no):
      single_stage_retrieval: no
